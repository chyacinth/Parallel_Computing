#!/bin/bash
#SBATCH -J myMPI            # job name
#SBATCH -o myMPIScan1M.o        # output and error file name (%j expands to jobID)
#SBATCH -N 1                # number of nodes requested
#SBATCH -n 1               # total number of mpi tasks requested
#SBATCH -p normal      # queue (partition) -- normal, development, etc.
#SBATCH -t 01:30:00         # run time (hh:mm:ss) - 1.5 hours

# Slurm email notifications are now working on Lonestar 5 
#SBATCH --mail-user=chyacinthz@gmail.com
#SBATCH --mail-type=begin   # email me when the job starts
#SBATCH --mail-type=end     # email me when the job finishes

# run the executable named a.out
# 1 core, 1 socket, 1 node

# 1M
ibrun numactl -C 12 ./a.out 1000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11 ./a.out 1000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 ./a.out 1000000

# 10M
ibrun numactl -C 6 ./a.out 10000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11 ./a.out 10000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 ./a.out 10000000

# 100M
ibrun numactl -C 18 ./a.out 100000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11 ./a.out 100000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 ./a.out 100000000

# 1B
ibrun numactl -C 22 ./a.out 1000000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11 ./a.out 1000000000
ibrun numactl -C 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 ./a.out 1000000000